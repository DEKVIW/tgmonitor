from telethon import TelegramClient, events
from sqlalchemy.orm import Session
from app.models.models import Message, engine, Channel, Credential
import datetime
import json
import re
import sys
from app.models.config import settings
import asyncio
from telethon.tl.types import MessageEntityTextUrl, MessageEntityUrl, KeyboardButtonUrl
from urllib.parse import unquote, urlparse
from urlextract import URLExtract  # æ–°å¢
from app.models.db import async_session

def get_api_credentials():
    """è·å– API å‡­æ®ï¼Œä¼˜å…ˆä½¿ç”¨æ•°æ®åº“ä¸­çš„å‡­æ®"""
    with Session(engine) as session:
        # å°è¯•ä»æ•°æ®åº“è·å–å‡­æ®
        cred = session.query(Credential).first()
        if cred:
            return int(cred.api_id), cred.api_hash
    # å¦‚æœæ•°æ®åº“ä¸­æ²¡æœ‰å‡­æ®ï¼Œä½¿ç”¨ .env ä¸­çš„é…ç½®
    return settings.TELEGRAM_API_ID, settings.TELEGRAM_API_HASH

def get_channels():
    """è·å–é¢‘é“åˆ—è¡¨ï¼Œåˆå¹¶æ•°æ®åº“å’Œ .env ä¸­çš„é¢‘é“"""
    channels = set()
    
    # ä»æ•°æ®åº“è·å–é¢‘é“
    with Session(engine) as session:
        db_channels = [c.username for c in session.query(Channel).all()]
        channels.update(db_channels)
    
    # ä» .env è·å–é»˜è®¤é¢‘é“
    if hasattr(settings, 'DEFAULT_CHANNELS'):
        env_channels = [c.strip() for c in settings.DEFAULT_CHANNELS.split(',') if c.strip()]
        channels.update(env_channels)
        
        # å°† .env ä¸­çš„é¢‘é“æ·»åŠ åˆ°æ•°æ®åº“
        with Session(engine) as session:
            for username in env_channels:
                if username not in db_channels:
                    channel = Channel(username=username)
                    session.add(channel)
            session.commit()
    
    return list(channels)

# è·å– API å‡­æ®
api_id, api_hash = get_api_credentials()

# session æ–‡ä»¶å
client = TelegramClient('newquark_session', api_id, api_hash)

# è·å–é¢‘é“åˆ—è¡¨
channel_usernames = get_channels()

# æ–°å¢ï¼šå…¨é¢æå–æ‰€æœ‰é“¾æ¥çš„å‡½æ•°
def extract_all_urls(text, msg_obj=None):
    extractor = URLExtract()
    all_urls = set()
    if msg_obj is not None:
        # å®ä½“é“¾æ¥
        for ent, text_part in msg_obj.get_entities_text():
            if isinstance(ent, MessageEntityTextUrl):
                decoded_url = unquote(ent.url)
                all_urls.add(decoded_url)
            elif isinstance(ent, MessageEntityUrl):
                decoded_url = unquote(text_part)
                all_urls.add(decoded_url)
        # æŒ‰é’®é“¾æ¥
        if getattr(msg_obj, 'reply_markup', None):
            for row in msg_obj.reply_markup.rows:
                for button in row.buttons:
                    if isinstance(button, KeyboardButtonUrl):
                        decoded_url = unquote(button.url)
                        all_urls.add(decoded_url)
        # ç½‘é¡µé¢„è§ˆé“¾æ¥
        if getattr(msg_obj, 'media', None) and hasattr(msg_obj.media, 'webpage'):
            webpage = msg_obj.media.webpage
            if hasattr(webpage, 'url') and webpage.url:
                decoded_url = unquote(webpage.url)
                all_urls.add(decoded_url)
    # è£¸é“¾å…œåº•
    for line in text.split('\n'):
        for url in extractor.find_urls(line):
            decoded_url = unquote(url)
            all_urls.add(decoded_url)
    return all_urls

def parse_message(text, msg_obj=None):
    original_lines = text.split('\n')
    lines_to_process = []
    title = ''
    description = ''
    tags = []
    source = ''
    channel = ''
    group = ''
    bot = ''
    desc_lines_buffer = []
    last_label = None
    label_pattern = re.compile(r'^(ä¸»é“¾|å¤‡ç”¨|æ™®ç |é«˜ç |HDR|æœæ¯”|IQ|[\u4e00-\u9fa5A-Za-z0-9]+ç )$')
    tag_pattern = re.compile(r'#([\u4e00-\u9fa5A-Za-z0-9_]+)')
    netdisk_map = [
        (['quark', 'å¤¸å…‹'], 'å¤¸å…‹ç½‘ç›˜'),
        (['aliyundrive', 'aliyun', 'é˜¿é‡Œ', 'alipan'], 'é˜¿é‡Œäº‘ç›˜'),
        (['baidu', 'pan.baidu'], 'ç™¾åº¦ç½‘ç›˜'),
        (['115.com', '115ç½‘ç›˜', '115pan', '115', '115cdn.com'], '115ç½‘ç›˜'),
        (['cloud.189', 'å¤©ç¿¼', '189.cn'], 'å¤©ç¿¼äº‘ç›˜'),
        (['123pan.com', 'www.123pan.com', '123912.com', 'www.123912.com', '123'], '123äº‘ç›˜'),
        (['ucdisk', 'ucç½‘ç›˜', 'ucloud', 'drive.uc.cn'], 'UCç½‘ç›˜'),
        (['xunlei', 'thunder', 'è¿…é›·'], 'è¿…é›·'),
    ]
    # 1. å…¨é‡æå–æ‰€æœ‰é“¾æ¥
    all_urls = extract_all_urls(text, msg_obj)
    # 2. åˆ†ç±»ä¸ºç½‘ç›˜é“¾æ¥
    links = {}
    valid_labels = {
        'æ™®ç ', 'é«˜ç ', 'ä¸»é“¾', 'å¤‡ç”¨', '4K', 'HDR', 'SDR', '1080P', '4K 120FPS', '4K HDR', '4K HQ', '4K EDR', '4K DV', '4K SDR', '4K 60FPS', '4K 120FPS', '4K HQ é«˜ç ç‡', 'å‰ 42 é›†', 'ATVP', '1080P 5.96G', '4K HDR 60FPS', '4K HQ', '4K DV', '4K EDR', '4K 5.96G', '4K 14.9GB', '4K 8.5GB', '4K 24.1GB', '4K HDR&DV', '4K HDR', '4K 60FPS', '4K 120FPS', '4K HQ é«˜ç ç‡', '4K HQ', '4K DV', '4K EDR', '4K 5.96G', '4K 14.9GB', '4K 8.5GB', '4K 24.1GB', 'ATVP', 'å‰ 42 é›†', 'ä¸»é“¾', 'å¤‡ç”¨',
        'å¤§åŒ…', 'å¤§åŒ…2', 'å¤§åŒ…3', 'å¤§åŒ…4', 'å¤§åŒ…5',
        '1å·æ–‡ä»¶å¤¹', '2å·æ–‡ä»¶å¤¹', '3å·æ–‡ä»¶å¤¹', '4å·æ–‡ä»¶å¤¹', '5å·æ–‡ä»¶å¤¹',
        'å¤‡ç”¨é“¾', 'å¤‡ç”¨é“¾æ¥', 'æ™®ç ç‰ˆ', 'é«˜ç ç‰ˆ', 'æ ‡å‡†ç‰ˆ', 'é«˜æ¸…ç‰ˆ',
        '4Kç‰ˆ', '1080Pç‰ˆ', 'HDRç‰ˆ', 'æœæ¯”ç‰ˆ', 'å®Œæ•´ç‰ˆ', 'ç²¾ç®€ç‰ˆ',
        'å¯¼æ¼”ç‰ˆ', 'åŠ é•¿ç‰ˆ', 'å›½è¯­ç‰ˆ', 'ç²¤è¯­ç‰ˆ', 'è‹±è¯­ç‰ˆ', 'å¤šè¯­ç‰ˆ',
        'æ— åˆ å‡', 'å‰§åœºç‰ˆ', 'ç‰¹åˆ«ç‰ˆ', 'å…¸è—ç‰ˆ', 'è±ªåç‰ˆ'
    }
    for url in all_urls:
        parsed = urlparse(url)
        netloc = parsed.netloc.lower()
        for keys, name in netdisk_map:
            if any(k in netloc for k in keys):
                # æ ‡ç­¾æå–é€»è¾‘
                label = None
                for i, line in enumerate(original_lines):
                    if url in line:
                        # å…ˆå°è¯•åŒ¹é…æœ‰å†’å·çš„æ ¼å¼
                        label_match = re.match(r'^([\u4e00-\u9fa5A-Za-z0-9]+)[ï¼š:]', line.strip())
                        if label_match:
                            extracted_label = label_match.group(1)
                            # ä¼˜å…ˆæœ€é•¿åŒ¹é…
                            matched_label = None
                            for valid_label in valid_labels:
                                if valid_label in extracted_label:
                                    if matched_label is None or len(valid_label) > len(matched_label):
                                        matched_label = valid_label
                            if matched_label:
                                label = matched_label
                                break
                        # å¦‚æœæ²¡æœ‰å†’å·ï¼Œå°è¯•åŒ¹é…é“¾æ¥å‰çš„æ ‡ç­¾
                        else:
                            url_index = line.find(url)
                            if url_index > 0:
                                before_url = line[:url_index].strip()
                                for valid_label in valid_labels:
                                    if before_url.endswith(valid_label):
                                        label = valid_label
                                        break
                                if label:
                                    break
                        # æ–°å¢ï¼šä¸Šä¸€è¡ŒçŸ­æ ‡ç­¾æ™ºèƒ½è¯†åˆ«
                        if not label and i > 0:
                            prev_line = original_lines[i-1].strip()
                            if len(prev_line) < 10:
                                for valid_label in valid_labels:
                                    if valid_label in prev_line:
                                        label = valid_label
                                        break
                        # åªè¦æ‰¾åˆ°å°±break
                        if label:
                            break
                if name not in links:
                    links[name] = []
                if not any(item['url'] == url for item in links[name]):
                    links[name].append({'label': label, 'url': url})
                break
    # å…¶ä½™ä¸šåŠ¡é€»è¾‘ä¿æŒä¸å˜ï¼ˆæ ‡é¢˜ã€æè¿°ã€æ ‡ç­¾ç­‰ï¼‰
    # é˜¶æ®µ1: ç²¾ç¡®è¯†åˆ«æ ‡é¢˜ï¼Œå¹¶å‡†å¤‡å¾…å¤„ç†è¡Œåˆ—è¡¨
    title_found_in_pass = False
    for i, line in enumerate(original_lines):
        stripped_line = line.strip()
        if not stripped_line:
            continue
        if stripped_line.startswith('åç§°ï¼š'):
            title = stripped_line.replace('åç§°ï¼š', '').strip()
            title_found_in_pass = True
            lines_to_process.extend(original_lines[:i])
            lines_to_process.extend(original_lines[i+1:])
            break
    if not title_found_in_pass and original_lines:
        first_meaningful_line_idx = -1
        for i, line in enumerate(original_lines):
            if line.strip():
                first_meaningful_line_idx = i
                break
        if first_meaningful_line_idx != -1:
            title = original_lines[first_meaningful_line_idx].strip()
            lines_to_process.extend(original_lines[:first_meaningful_line_idx])
            lines_to_process.extend(original_lines[first_meaningful_line_idx+1:])
        else:
            return {'title': '', 'description': '', 'links': {}, 'tags': [], 'source': '', 'channel': '', 'group_name': '', 'bot': ''}
    # ç»Ÿä¸€å®šä¹‰éœ€è¦è¿‡æ»¤çš„â€œæ‚é¡¹è¡Œâ€å…³é”®è¯åŠå…¶å¯¹åº”å­—æ®µ
    skip_keywords = [
        ('ğŸ‰ æ¥è‡ª', 'source'),
        ('ğŸ“¢ é¢‘é“', 'channel'),
        ('ğŸ‘¥ ç¾¤ç»„', 'group'),
        ('ğŸ¤– æŠ•ç¨¿', 'bot'),
        ('ğŸ” æŠ•ç¨¿/æœç´¢', None),
        ('âš ï¸', None)
    ]
    skip_pattern = re.compile(r'^(%s)(ï¼š|:)?' % '|'.join(map(lambda x: re.escape(x[0]), skip_keywords)))
    keyword_field_map = {k: v for k, v in skip_keywords if v}
    extractor_tmp = URLExtract()
    for raw_line in lines_to_process:
        line = raw_line.strip()
        if not line:
            continue
        # æ–°å¢ï¼šåªè¦è¡Œé‡Œå«æœ‰ä»»ä½•å½¢å¼çš„é“¾æ¥ï¼ˆåŒ…å«ä¸å¸¦ http/https å‰ç¼€çš„è£¸åŸŸåï¼‰ï¼Œæ•´è¡Œè·³è¿‡
        if re.search(r'https?://', line) or extractor_tmp.has_urls(line):
            continue
        # æ–°å¢ï¼šè¿‡æ»¤åŒ…å« @xxx çš„è¡Œ
        if re.search(r'@[A-Za-z0-9_]+', line):
            continue
        cleaned_line_for_check = re.sub(r'^(?:\* |\- |\+ |> |>> |â€¢ |â¤ |â–ª |âˆš )+', '', line).strip()
        line_fully_handled = False
        m = skip_pattern.match(cleaned_line_for_check)
        if m:
            keyword = m.group(1)
            field = keyword_field_map.get(keyword)
            if field:
                value = cleaned_line_for_check.replace(keyword, '').replace('ï¼š', '').replace(':', '').strip()
                locals()[field] = value
            continue
        # ä»…è¡Œé¦–å¤§å°ä¿¡æ¯è¯†åˆ«ï¼ˆå…è®¸emojiã€ç©ºæ ¼ã€æ ‡ç‚¹ï¼‰
        if re.match(r'^[^\u4e00-\u9fa5A-Za-z0-9]*å¤§å°', cleaned_line_for_check):
            parts = re.split(r'å¤§å°[:ï¼š\s]*', cleaned_line_for_check, maxsplit=1)
            size_info = parts[1].strip() if len(parts) > 1 else ""
            if re.search(r'(\d+\s*(GB|MB|TB|KB|G|M|T|K|B|å­—èŠ‚|å·¦å³|çº¦|æ¯é›†|å•é›†))', size_info, re.IGNORECASE):
                desc_lines_buffer.append(cleaned_line_for_check)
            continue
        elif cleaned_line_for_check.startswith('é“¾æ¥ï¼š'):
            continue
        elif cleaned_line_for_check.startswith('æè¿°åŒºåŸŸ'):
            continue
        elif label_pattern.match(cleaned_line_for_check):
            last_label = cleaned_line_for_check
            continue
        if line_fully_handled:
            continue
        cleaned_line = cleaned_line_for_check
        cleaned_line = re.sub(r'\bvia\s*\S+', '', cleaned_line, flags=re.IGNORECASE).strip()
        cleaned_line = re.sub(r'\bvia\s*$', '', cleaned_line, flags=re.IGNORECASE).strip() 
        found_tags_in_line = tag_pattern.findall(cleaned_line)
        if found_tags_in_line:
            tags.extend(found_tags_in_line)
            cleaned_line = tag_pattern.sub('', cleaned_line).strip()
        cleaned_line = re.sub(r'^.*(æ ‡ç­¾|æŠ•ç¨¿äºº|é¢‘é“|æœç´¢|æœºåœº)\s*[ï¼š:].*$', '', cleaned_line, flags=re.IGNORECASE).strip()
        if cleaned_line_for_check.startswith('åˆ†äº«ï¼š') or cleaned_line_for_check.startswith('ç½‘å€ï¼š') \
            or cleaned_line_for_check.startswith('ğŸŒ') or cleaned_line_for_check.startswith('ğŸ”¥'):
            continue
        cleaned_line = re.sub(r'[ğŸ”—\s]*é“¾æ¥[ï¼š:ï¼š]?\s*[^\s]+', '', cleaned_line).strip()
        if cleaned_line:
            filter_patterns = [
                r'.*ğŸŒ.*ç¾¤ä¸»è‡ªç”¨æœºåœº.*å®ˆå€™ç½‘ç»œ.*9æŠ˜æ´»åŠ¨.*',
                r'.*ğŸ”¥.*äº‘ç›˜æ’­æ”¾ç¥å™¨.*VidHub.*',
                r'.*ç¾¤ä¸»è‡ªç”¨æœºåœº.*å®ˆå€™ç½‘ç»œ.*9æŠ˜æ´»åŠ¨.*',
                r'.*äº‘ç›˜æ’­æ”¾ç¥å™¨.*VidHub.*'
            ]
            should_filter = False
            for pattern in filter_patterns:
                if re.search(pattern, cleaned_line, re.IGNORECASE):
                    should_filter = True
                    break
            if not should_filter:
                desc_lines_buffer.append(cleaned_line)
    tags = list(set(tags))
    description = '\n'.join(desc_lines_buffer)
    # --- æ–°å¢ï¼šæè¿°åŒºå‡€åŒ–ï¼Œå»é™¤ç½‘ç›˜å ---
    netdisk_names = ['å¤¸å…‹', 'è¿…é›·', 'ç™¾åº¦', 'UC', 'é˜¿é‡Œ', 'å¤©ç¿¼', '115', '123äº‘ç›˜']
    netdisk_name_pattern = re.compile(r'(' + '|'.join(netdisk_names) + r')')
    description = netdisk_name_pattern.sub('', description)
    # é“¾æ¥ç›¸å…³çš„replaceå·²ä¸éœ€è¦ï¼Œç›´æ¥åˆ é™¤
    description = re.sub(r'ï¼š\s*$', '', description, flags=re.MULTILINE)
    description = re.sub(r'ï¼š\s*\n', '\n', description, flags=re.MULTILINE)
    desc_lines_final = [line for line in description.strip().split('\n') if line.strip() and not re.fullmatch(r'[.ã€‚Â·ã€,ï¼Œ-]+', line.strip())]
    description = '\n'.join(desc_lines_final)
    return {
        'title': title,
        'description': description,
        'links': links,
        'tags': tags,
        'source': source,
        'channel': channel,
        'group_name': group,
        'bot': bot
    }

@client.on(events.NewMessage(chats=channel_usernames))
async def handler(event):
    try:
        message = event.raw_text
        # ä½¿ç”¨Telegramæ¶ˆæ¯çš„åŸå§‹æ—¶é—´ï¼Œæ­£ç¡®å¤„ç†æ—¶åŒº
        telegram_time = event.date
        monitor_time = datetime.datetime.now()
        
        # æ­£ç¡®å¤„ç†æ—¶åŒºè½¬æ¢
        if telegram_time.tzinfo is not None:
            # å¦‚æœtelegram_timeæœ‰æ—¶åŒºä¿¡æ¯ï¼Œè½¬æ¢ä¸ºæœ¬åœ°æ—¶é—´
            # å‡è®¾Telegramæ—¶é—´æ˜¯UTCï¼Œè½¬æ¢ä¸ºåŒ—äº¬æ—¶é—´ï¼ˆUTC+8ï¼‰
            telegram_local_time = telegram_time.replace(tzinfo=None) + datetime.timedelta(hours=8)
        else:
            # å¦‚æœæ²¡æœ‰æ—¶åŒºä¿¡æ¯ï¼Œç›´æ¥ä½¿ç”¨
            telegram_local_time = telegram_time
        
        # è®¡ç®—ç›‘æ§å»¶è¿Ÿ
        delay_seconds = (monitor_time - telegram_local_time).total_seconds()
        
        print(f"[{monitor_time}] æ”¶åˆ°æ–°æ¶ˆæ¯ï¼Œå¼€å§‹è§£æ... (å»¶è¿Ÿ: {delay_seconds:.1f}ç§’)")
        
        # è§£ææ¶ˆæ¯
        try:
            parsed_data = parse_message(message, event.message) # Pass event.message to parse_message
        except Exception as parse_error:
            print(f"[{monitor_time}] æ¶ˆæ¯è§£æå¤±è´¥: {str(parse_error)}")
            print(f"[{monitor_time}] åŸå§‹æ¶ˆæ¯: {message[:200]}...")  # è®°å½•å‰200å­—ç¬¦
            return
        
        # åªä¿å­˜åŒ…å«ç½‘ç›˜é“¾æ¥çš„æ¶ˆæ¯
        if parsed_data['links']:
            netdisk_types = list(parsed_data['links'].keys())
            max_retries = 3
            for attempt in range(max_retries):
                try:
                    async with async_session() as session:
                        new_message = Message(
                            timestamp=telegram_local_time,  # ä½¿ç”¨è½¬æ¢åçš„æœ¬åœ°æ—¶é—´
                            **parsed_data,
                            netdisk_types=netdisk_types
                        )
                        session.add(new_message)
                        await session.commit()
                    print(f"[{monitor_time}] æ–°æ¶ˆæ¯å·²ä¿å­˜åˆ°æ•°æ®åº“ (å°è¯• {attempt + 1}/{max_retries}, å»¶è¿Ÿ: {delay_seconds:.1f}ç§’)")
                    break
                except Exception as db_error:
                    print(f"[{monitor_time}] æ•°æ®åº“å†™å…¥å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {str(db_error)}")
                    if attempt == max_retries - 1:
                        print(f"[{monitor_time}] æ•°æ®åº“å†™å…¥æœ€ç»ˆå¤±è´¥ï¼Œæ¶ˆæ¯ä¸¢å¤±")
                        # å¯ä»¥è€ƒè™‘å†™å…¥æœ¬åœ°æ–‡ä»¶ä½œä¸ºå¤‡ä»½
                        try:
                            with open('data/failed_messages.log', 'a', encoding='utf-8') as f:
                                f.write(f"[{monitor_time}] å¤±è´¥æ¶ˆæ¯: {message}\n")
                        except:
                            pass
                    else:
                        await asyncio.sleep(1)  # ç”¨å¼‚æ­¥sleepæ›¿æ¢é˜»å¡sleep
        else:
            print(f"[{monitor_time}] è¿‡æ»¤æ‰æ— ç½‘ç›˜é“¾æ¥çš„æ¶ˆæ¯")
            
    except Exception as e:
        print(f"[{datetime.datetime.now()}] æ¶ˆæ¯å¤„ç†å‘ç”ŸæœªçŸ¥é”™è¯¯: {str(e)}")
        # è®°å½•åŸå§‹æ¶ˆæ¯åˆ°é”™è¯¯æ—¥å¿—
        try:
            with open('data/error_messages.log', 'a', encoding='utf-8') as f:
                f.write(f"[{datetime.datetime.now()}] é”™è¯¯: {str(e)}, æ¶ˆæ¯: {event.raw_text[:200]}...\n")
        except:
            pass

print(f"âœ… æ­£åœ¨ç›‘å¬ Telegram é¢‘é“ï¼š{channel_usernames} ...")

# æ·»åŠ è¿æ¥çŠ¶æ€ç›‘æ§
@client.on(events.Raw)
async def connection_handler(event):
    """ç›‘æ§è¿æ¥çŠ¶æ€"""
    if hasattr(event, 'connected'):
        if event.connected:
            print(f"[{datetime.datetime.now()}] âœ… Telegramè¿æ¥å·²å»ºç«‹")
        else:
            print(f"[{datetime.datetime.now()}] âŒ Telegramè¿æ¥å·²æ–­å¼€")

if __name__ == "__main__":
    try:
        # ä½¿ç”¨å·²å­˜åœ¨çš„ session æ–‡ä»¶å¯åŠ¨
        client.start()
        print(f"[{datetime.datetime.now()}] âœ… ç›‘æ§æœåŠ¡å¯åŠ¨æˆåŠŸ")
        client.run_until_disconnected()
    except Exception as e:
        print(f"[{datetime.datetime.now()}] âŒ å¯åŠ¨å¤±è´¥: {str(e)}")
        print("è¯·å…ˆæ‰‹åŠ¨è¿è¡Œä¸€æ¬¡ç¨‹åºè¿›è¡Œç™»å½•ï¼špython monitor.py")
        sys.exit(1) 
from telethon import TelegramClient, events
from sqlalchemy.orm import Session
from models import Message, engine, Channel, Credential
import datetime
import json
import re
import sys
from config import settings
import asyncio
from telethon.tl.types import MessageEntityTextUrl, MessageEntityUrl
from urllib.parse import unquote
from urlextract import URLExtract  # æ–°å¢

def get_api_credentials():
    """è·å– API å‡­æ®ï¼Œä¼˜å…ˆä½¿ç”¨æ•°æ®åº“ä¸­çš„å‡­æ®"""
    with Session(engine) as session:
        # å°è¯•ä»æ•°æ®åº“è·å–å‡­æ®
        cred = session.query(Credential).first()
        if cred:
            return int(cred.api_id), cred.api_hash
    # å¦‚æœæ•°æ®åº“ä¸­æ²¡æœ‰å‡­æ®ï¼Œä½¿ç”¨ .env ä¸­çš„é…ç½®
    return settings.TELEGRAM_API_ID, settings.TELEGRAM_API_HASH

def get_channels():
    """è·å–é¢‘é“åˆ—è¡¨ï¼Œåˆå¹¶æ•°æ®åº“å’Œ .env ä¸­çš„é¢‘é“"""
    channels = set()
    
    # ä»æ•°æ®åº“è·å–é¢‘é“
    with Session(engine) as session:
        db_channels = [c.username for c in session.query(Channel).all()]
        channels.update(db_channels)
    
    # ä» .env è·å–é»˜è®¤é¢‘é“
    if hasattr(settings, 'DEFAULT_CHANNELS'):
        env_channels = [c.strip() for c in settings.DEFAULT_CHANNELS.split(',') if c.strip()]
        channels.update(env_channels)
        
        # å°† .env ä¸­çš„é¢‘é“æ·»åŠ åˆ°æ•°æ®åº“
        with Session(engine) as session:
            for username in env_channels:
                if username not in db_channels:
                    channel = Channel(username=username)
                    session.add(channel)
            session.commit()
    
    return list(channels)

# è·å– API å‡­æ®
api_id, api_hash = get_api_credentials()

# session æ–‡ä»¶å
client = TelegramClient('newquark_session', api_id, api_hash)

# è·å–é¢‘é“åˆ—è¡¨
channel_usernames = get_channels()

def parse_message(text, msg_obj=None):
    original_lines = text.split('\n')
    lines_to_process = []
    title = ''
    # å­—æ®µåˆå§‹åŒ–
    description = ''
    links = {}
    tags = []
    source = ''
    channel = ''
    group = ''
    bot = ''
    desc_lines_buffer = [] 
    last_label = None
    # åŒºåˆ†è¯åˆ—è¡¨
    label_pattern = re.compile(r'^(ä¸»é“¾|å¤‡ç”¨|æ™®ç |é«˜ç |HDR|æœæ¯”|IQ|[\u4e00-\u9fa5A-Za-z0-9]+ç )$')
    # æ ‡ç­¾æ­£åˆ™è¡¨è¾¾å¼
    tag_pattern = re.compile(r'#([\u4e00-\u9fa5A-Za-z0-9_]+)')
    # ç½‘ç›˜å…³é”®å­—ä¸æ˜¾ç¤ºåæ˜ å°„
    netdisk_map = [
        (['quark', 'å¤¸å…‹'], 'å¤¸å…‹ç½‘ç›˜'),
        (['aliyundrive', 'aliyun', 'é˜¿é‡Œ', 'alipan'], 'é˜¿é‡Œäº‘ç›˜'),
        (['baidu', 'pan.baidu'], 'ç™¾åº¦ç½‘ç›˜'),
        (['115.com', '115ç½‘ç›˜', '115pan', '115', '115cdn.com'], '115ç½‘ç›˜'),
        (['cloud.189', 'å¤©ç¿¼', '189.cn'], 'å¤©ç¿¼äº‘ç›˜'),
        (['123pan.com', 'www.123pan.com', '123912.com', 'www.123912.com', '123'], '123äº‘ç›˜'),
        (['ucdisk', 'ucç½‘ç›˜', 'ucloud', 'drive.uc.cn'], 'UCç½‘ç›˜'),
        (['xunlei', 'thunder', 'è¿…é›·'], 'è¿…é›·'),
    ]
    # --- æ–°å¢ï¼šç”¨ urlextract æå–æ‰€æœ‰é“¾æ¥ ---
    extractor = URLExtract()
    all_urls = set()
    if msg_obj is not None:
        # 1. ç”¨Telethonå®ä½“æå–
        for ent, text in msg_obj.get_entities_text():
            if isinstance(ent, MessageEntityTextUrl):
                decoded_url = unquote(ent.url)
                all_urls.add(decoded_url)
            elif isinstance(ent, MessageEntityUrl):
                decoded_url = unquote(text)
                all_urls.add(decoded_url)
    # 2. å…œåº•ï¼šç”¨ urlextract æå–è£¸é“¾æ¥
    for line in original_lines:
        for url in extractor.find_urls(line):
            decoded_url = unquote(url)
            all_urls.add(decoded_url)
    # --- é˜¶æ®µ1: ç²¾ç¡®è¯†åˆ«æ ‡é¢˜ï¼Œå¹¶å‡†å¤‡å¾…å¤„ç†è¡Œåˆ—è¡¨ ---
    title_found_in_pass = False
    for i, line in enumerate(original_lines):
        stripped_line = line.strip()
        if not stripped_line:
            continue
        if stripped_line.startswith('åç§°ï¼š'):
            title = stripped_line.replace('åç§°ï¼š', '').strip()
            title_found_in_pass = True
            lines_to_process.extend(original_lines[:i])
            lines_to_process.extend(original_lines[i+1:])
            break
    if not title_found_in_pass and original_lines:
        first_meaningful_line_idx = -1
        for i, line in enumerate(original_lines):
            if line.strip():
                first_meaningful_line_idx = i
                break
        if first_meaningful_line_idx != -1:
            title = original_lines[first_meaningful_line_idx].strip()
            lines_to_process.extend(original_lines[:first_meaningful_line_idx])
            lines_to_process.extend(original_lines[first_meaningful_line_idx+1:])
        else:
            return {'title': '', 'description': '', 'links': {}, 'tags': [], 'source': '', 'channel': '', 'group_name': '', 'bot': ''}
    # ç»Ÿä¸€å®šä¹‰éœ€è¦è¿‡æ»¤çš„â€œæ‚é¡¹è¡Œâ€å…³é”®è¯åŠå…¶å¯¹åº”å­—æ®µ
    skip_keywords = [
        ('ğŸ‰ æ¥è‡ª', 'source'),
        ('ğŸ“¢ é¢‘é“', 'channel'),
        ('ğŸ‘¥ ç¾¤ç»„', 'group'),
        ('ğŸ¤– æŠ•ç¨¿', 'bot'),
        ('ğŸ” æŠ•ç¨¿/æœç´¢', None),
        ('âš ï¸', None)
    ]
    skip_pattern = re.compile(r'^(%s)(ï¼š|:)?' % '|'.join(map(lambda x: re.escape(x[0]), skip_keywords)))
    keyword_field_map = {k: v for k, v in skip_keywords if v}
    # --- é˜¶æ®µ2: éå†å¾…å¤„ç†è¡Œï¼Œæå–å…ƒæ•°æ®å¹¶æ„å»ºçº¯å‡€æè¿° ---
    for raw_line in lines_to_process:
        line = raw_line.strip()
        if not line:
            continue
        cleaned_line_for_check = re.sub(r'^(?:\* |\- |\+ |> |>> |â€¢ |â¤ |â–ª |âˆš )+', '', line).strip()
        line_fully_handled = False
        m = skip_pattern.match(cleaned_line_for_check)
        if m:
            keyword = m.group(1)
            field = keyword_field_map.get(keyword)
            if field:
                value = cleaned_line_for_check.replace(keyword, '').replace('ï¼š', '').replace(':', '').strip()
                locals()[field] = value
            continue
        elif cleaned_line_for_check.startswith('ğŸ“ å¤§å°ï¼š') or cleaned_line_for_check.startswith('å¤§å°ï¼š'):
            size_info = cleaned_line_for_check.replace('ğŸ“ å¤§å°ï¼š', '').replace('å¤§å°ï¼š', '').strip()
            if re.search(r'(\d+\s*(GB|MB|TB|KB|G|M|T|K|B|å­—èŠ‚|å·¦å³|çº¦|æ¯é›†|å•é›†))', size_info, re.IGNORECASE):
                desc_lines_buffer.append(cleaned_line_for_check)
            continue
        elif cleaned_line_for_check.startswith('é“¾æ¥ï¼š'):
            continue
        elif cleaned_line_for_check.startswith('æè¿°åŒºåŸŸ'):
            continue
        elif label_pattern.match(cleaned_line_for_check):
            last_label = cleaned_line_for_check
            continue
        if line_fully_handled:
            continue
        cleaned_line = cleaned_line_for_check
        cleaned_line = re.sub(r'\bvia\s*\S+', '', cleaned_line, flags=re.IGNORECASE).strip()
        cleaned_line = re.sub(r'\bvia\s*$', '', cleaned_line, flags=re.IGNORECASE).strip() 
        found_tags_in_line = tag_pattern.findall(cleaned_line)
        if found_tags_in_line:
            tags.extend(found_tags_in_line)
            cleaned_line = tag_pattern.sub('', cleaned_line).strip()
        cleaned_line = re.sub(r'(?:ğŸ“\s*)?å¤§å°\s*[ï¼š:]\s*(?:N|X|æ— |æœªçŸ¥)', '', cleaned_line, flags=re.IGNORECASE).strip()
        cleaned_line = re.sub(r'^.*(æ ‡ç­¾|æŠ•ç¨¿äºº|é¢‘é“|æœç´¢|æœºåœº)\s*[ï¼š:].*$', '', cleaned_line, flags=re.IGNORECASE).strip()
        if cleaned_line_for_check.startswith('åˆ†äº«ï¼š') or cleaned_line_for_check.startswith('ç½‘å€ï¼š') \
            or cleaned_line_for_check.startswith('ğŸŒ') or cleaned_line_for_check.startswith('ğŸ”¥'):
            continue
        cleaned_line = re.sub(r'[ğŸ”—\s]*é“¾æ¥[ï¼š:ï¼š]?\s*[^\s]+', '', cleaned_line).strip()
        if cleaned_line:
            filter_patterns = [
                r'.*ğŸŒ.*ç¾¤ä¸»è‡ªç”¨æœºåœº.*å®ˆå€™ç½‘ç»œ.*9æŠ˜æ´»åŠ¨.*',
                r'.*ğŸ”¥.*äº‘ç›˜æ’­æ”¾ç¥å™¨.*VidHub.*',
                r'.*ç¾¤ä¸»è‡ªç”¨æœºåœº.*å®ˆå€™ç½‘ç»œ.*9æŠ˜æ´»åŠ¨.*',
                r'.*äº‘ç›˜æ’­æ”¾ç¥å™¨.*VidHub.*'
            ]
            should_filter = False
            for pattern in filter_patterns:
                if re.search(pattern, cleaned_line, re.IGNORECASE):
                    should_filter = True
                    break
            if not should_filter:
                desc_lines_buffer.append(cleaned_line)
    valid_labels = {
        'æ™®ç ', 'é«˜ç ', 'ä¸»é“¾', 'å¤‡ç”¨', '4K', 'HDR', 'SDR', '1080P', '4K 120FPS', '4K HDR', '4K HQ', '4K EDR', '4K DV', '4K SDR', '4K 60FPS', '4K 120FPS', '4K HQ é«˜ç ç‡', 'å‰ 42 é›†', 'ATVP', '1080P 5.96G', '4K HDR 60FPS', '4K HQ', '4K DV', '4K EDR', '4K 5.96G', '4K 14.9GB', '4K 8.5GB', '4K 24.1GB', '4K HDR&DV', '4K HDR', '4K 60FPS', '4K 120FPS', '4K HQ é«˜ç ç‡', '4K HQ', '4K DV', '4K EDR', '4K 5.96G', '4K 14.9GB', '4K 8.5GB', '4K 24.1GB', 'ATVP', 'å‰ 42 é›†', 'ä¸»é“¾', 'å¤‡ç”¨',
        'å¤§åŒ…', 'å¤§åŒ…2', 'å¤§åŒ…3', 'å¤§åŒ…4', 'å¤§åŒ…5',
        '1å·æ–‡ä»¶å¤¹', '2å·æ–‡ä»¶å¤¹', '3å·æ–‡ä»¶å¤¹', '4å·æ–‡ä»¶å¤¹', '5å·æ–‡ä»¶å¤¹',
        'å¤‡ç”¨é“¾', 'å¤‡ç”¨é“¾æ¥', 'æ™®ç ç‰ˆ', 'é«˜ç ç‰ˆ', 'æ ‡å‡†ç‰ˆ', 'é«˜æ¸…ç‰ˆ',
        '4Kç‰ˆ', '1080Pç‰ˆ', 'HDRç‰ˆ', 'æœæ¯”ç‰ˆ', 'å®Œæ•´ç‰ˆ', 'ç²¾ç®€ç‰ˆ',
        'å¯¼æ¼”ç‰ˆ', 'åŠ é•¿ç‰ˆ', 'å›½è¯­ç‰ˆ', 'ç²¤è¯­ç‰ˆ', 'è‹±è¯­ç‰ˆ', 'å¤šè¯­ç‰ˆ',
        'æ— åˆ å‡', 'å‰§åœºç‰ˆ', 'ç‰¹åˆ«ç‰ˆ', 'å…¸è—ç‰ˆ', 'è±ªåç‰ˆ'
    }
    # --- æ–°ä¸»å¾ªç¯ï¼šç”¨ urlextract æå–æ‰€æœ‰é“¾æ¥ï¼Œæ ‡ç­¾èµ‹å€¼é€»è¾‘å…¼å®¹æ‰€æœ‰æ ¼å¼ ---
    links = {}
    last_label = None
    for raw_line in original_lines:
        line = raw_line.strip()
        if not line:
            continue
        # æ•´è¡Œæ˜¯æœ‰æ•ˆæ ‡ç­¾ï¼Œèµ‹å€¼last_labelï¼Œè·³è¿‡æœ¬è¡Œ
        if line in valid_labels:
            last_label = line
            continue
        # æ£€æŸ¥â€œæ ‡ç­¾ï¼šé“¾æ¥â€æ ¼å¼
        label = None
        label_match = re.match(r'^([\u4e00-\u9fa5A-Za-z0-9]+)[ï¼š:]', line)
        if label_match:
            possible_label = label_match.group(1)
            if possible_label in valid_labels:
                label = possible_label
                line = line[label_match.end():].strip()
        urls_in_line = extractor.find_urls(line)
        for url in urls_in_line:
            decoded_url = unquote(url)
            for keys, name in netdisk_map:
                if any(k in decoded_url.lower() for k in keys):
                    use_label = label if label else last_label
                    if name not in links:
                        links[name] = []
                    if not any(item['url'] == decoded_url for item in links[name]):
                        links[name].append({'label': use_label if use_label in valid_labels else None, 'url': decoded_url})
                    last_label = None  # ç”¨å®Œå³æ¸…ç©ºï¼Œé˜²æ­¢æ®‹ç•™
                    break
    # --- æ–°å¢ï¼šå…¨è¡Œæ— æ¡ä»¶æå–+æ™ºèƒ½æ ‡ç­¾è¯†åˆ«ï¼Œåˆå¹¶åˆ°links ---
    def merge_link(links_dict, netdisk, url, label):
        if netdisk not in links_dict:
            links_dict[netdisk] = []
        for item in links_dict[netdisk]:
            if item['url'] == url:
                return
        links_dict[netdisk].append({'label': label, 'url': url})
    for raw_line in original_lines:
        line = raw_line.strip()
        if not line:
            continue
        label = None
        label_match = re.match(r'^([\u4e00-\u9fa5A-Za-z0-9]+)[ï¼š:]', line)
        if label_match:
            possible_label = label_match.group(1)
            if possible_label in valid_labels:
                label = possible_label
                line = line[label_match.end():].strip()
        urls_in_line = extractor.find_urls(line)
        for url in urls_in_line:
            decoded_url = unquote(url)
            for keys, name in netdisk_map:
                if any(k in decoded_url.lower() for k in keys):
                    merge_link(links, name, decoded_url, label)
                    break
    # æ•´åˆæœ€ç»ˆçš„æè¿°å’Œæ ‡ç­¾
    description = '\n'.join(desc_lines_buffer)
    netdisk_names = ['å¤¸å…‹', 'è¿…é›·', 'ç™¾åº¦', 'UC', 'é˜¿é‡Œ', 'å¤©ç¿¼', '115', '123äº‘ç›˜']
    netdisk_name_pattern = re.compile(r'(' + '|'.join(netdisk_names) + r')')
    description = netdisk_name_pattern.sub('', description)
    for netdisk_list in links.values():
        for item in netdisk_list:
            description = description.replace(item['url'], '')
            encoded_url = item['url'].replace('{', '%7B').replace('}', '%7D')
            description = description.replace(encoded_url, '')
    description = re.sub(r'ï¼š\s*$', '', description, flags=re.MULTILINE)
    description = re.sub(r'ï¼š\s*\n', '\n', description, flags=re.MULTILINE)
    desc_lines_final = [line for line in description.strip().split('\n') if line.strip() and not re.fullmatch(r'[.ã€‚Â·ã€,ï¼Œ-]+', line.strip())]
    description = '\n'.join(desc_lines_final)
    tags = list(set(tags))
    return {
        'title': title,
        'description': description,
        'links': links,
        'tags': tags,
        'source': source,
        'channel': channel,
        'group_name': group,
        'bot': bot
    }

@client.on(events.NewMessage(chats=channel_usernames))
async def handler(event):
    try:
        message = event.raw_text
        # ä½¿ç”¨Telegramæ¶ˆæ¯çš„åŸå§‹æ—¶é—´ï¼Œæ­£ç¡®å¤„ç†æ—¶åŒº
        telegram_time = event.date
        monitor_time = datetime.datetime.now()
        
        # æ­£ç¡®å¤„ç†æ—¶åŒºè½¬æ¢
        if telegram_time.tzinfo is not None:
            # å¦‚æœtelegram_timeæœ‰æ—¶åŒºä¿¡æ¯ï¼Œè½¬æ¢ä¸ºæœ¬åœ°æ—¶é—´
            # å‡è®¾Telegramæ—¶é—´æ˜¯UTCï¼Œè½¬æ¢ä¸ºåŒ—äº¬æ—¶é—´ï¼ˆUTC+8ï¼‰
            telegram_local_time = telegram_time.replace(tzinfo=None) + datetime.timedelta(hours=8)
        else:
            # å¦‚æœæ²¡æœ‰æ—¶åŒºä¿¡æ¯ï¼Œç›´æ¥ä½¿ç”¨
            telegram_local_time = telegram_time
        
        # è®¡ç®—ç›‘æ§å»¶è¿Ÿ
        delay_seconds = (monitor_time - telegram_local_time).total_seconds()
        
        print(f"[{monitor_time}] æ”¶åˆ°æ–°æ¶ˆæ¯ï¼Œå¼€å§‹è§£æ... (å»¶è¿Ÿ: {delay_seconds:.1f}ç§’)")
        
        # è§£ææ¶ˆæ¯
        try:
            parsed_data = parse_message(message, event.message) # Pass event.message to parse_message
        except Exception as parse_error:
            print(f"[{monitor_time}] æ¶ˆæ¯è§£æå¤±è´¥: {str(parse_error)}")
            print(f"[{monitor_time}] åŸå§‹æ¶ˆæ¯: {message[:200]}...")  # è®°å½•å‰200å­—ç¬¦
            return
        
        # åªä¿å­˜åŒ…å«ç½‘ç›˜é“¾æ¥çš„æ¶ˆæ¯
        if parsed_data['links']:
            netdisk_types = list(parsed_data['links'].keys())
            max_retries = 3
            for attempt in range(max_retries):
                try:
                    with Session(engine) as session:
                        new_message = Message(
                            timestamp=telegram_local_time,  # ä½¿ç”¨è½¬æ¢åçš„æœ¬åœ°æ—¶é—´
                            **parsed_data,
                            netdisk_types=netdisk_types
                        )
                        session.add(new_message)
                        session.commit()
                    print(f"[{monitor_time}] æ–°æ¶ˆæ¯å·²ä¿å­˜åˆ°æ•°æ®åº“ (å°è¯• {attempt + 1}/{max_retries}, å»¶è¿Ÿ: {delay_seconds:.1f}ç§’)")
                    break
                except Exception as db_error:
                    print(f"[{monitor_time}] æ•°æ®åº“å†™å…¥å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {str(db_error)}")
                    if attempt == max_retries - 1:
                        print(f"[{monitor_time}] æ•°æ®åº“å†™å…¥æœ€ç»ˆå¤±è´¥ï¼Œæ¶ˆæ¯ä¸¢å¤±")
                        # å¯ä»¥è€ƒè™‘å†™å…¥æœ¬åœ°æ–‡ä»¶ä½œä¸ºå¤‡ä»½
                        try:
                            with open('data/failed_messages.log', 'a', encoding='utf-8') as f:
                                f.write(f"[{monitor_time}] å¤±è´¥æ¶ˆæ¯: {message}\n")
                        except:
                            pass
                    else:
                        await asyncio.sleep(1)  # ç”¨å¼‚æ­¥sleepæ›¿æ¢é˜»å¡sleep
        else:
            print(f"[{monitor_time}] è¿‡æ»¤æ‰æ— ç½‘ç›˜é“¾æ¥çš„æ¶ˆæ¯")
            
    except Exception as e:
        print(f"[{datetime.datetime.now()}] æ¶ˆæ¯å¤„ç†å‘ç”ŸæœªçŸ¥é”™è¯¯: {str(e)}")
        # è®°å½•åŸå§‹æ¶ˆæ¯åˆ°é”™è¯¯æ—¥å¿—
        try:
            with open('data/error_messages.log', 'a', encoding='utf-8') as f:
                f.write(f"[{datetime.datetime.now()}] é”™è¯¯: {str(e)}, æ¶ˆæ¯: {event.raw_text[:200]}...\n")
        except:
            pass

print(f"âœ… æ­£åœ¨ç›‘å¬ Telegram é¢‘é“ï¼š{channel_usernames} ...")

# æ·»åŠ è¿æ¥çŠ¶æ€ç›‘æ§
@client.on(events.Raw)
async def connection_handler(event):
    """ç›‘æ§è¿æ¥çŠ¶æ€"""
    if hasattr(event, 'connected'):
        if event.connected:
            print(f"[{datetime.datetime.now()}] âœ… Telegramè¿æ¥å·²å»ºç«‹")
        else:
            print(f"[{datetime.datetime.now()}] âŒ Telegramè¿æ¥å·²æ–­å¼€")

if __name__ == "__main__":
    try:
        # ä½¿ç”¨å·²å­˜åœ¨çš„ session æ–‡ä»¶å¯åŠ¨
        client.start()
        print(f"[{datetime.datetime.now()}] âœ… ç›‘æ§æœåŠ¡å¯åŠ¨æˆåŠŸ")
        client.run_until_disconnected()
    except Exception as e:
        print(f"[{datetime.datetime.now()}] âŒ å¯åŠ¨å¤±è´¥: {str(e)}")
        print("è¯·å…ˆæ‰‹åŠ¨è¿è¡Œä¸€æ¬¡ç¨‹åºè¿›è¡Œç™»å½•ï¼špython monitor.py")
        sys.exit(1) 